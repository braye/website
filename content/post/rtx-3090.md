---
title: "The RTX 3090 FE or: how I learned to stop worrying and love Tcase"
date: 2022-01-21T16:03:41-08:00
draft: false
---

<img
  src="/images/3090-chonk.jpg"
  alt="the absolute lad that is the rtx 3090 FE">

> This thing is as big as my 1st gen X1 Carbon!

It was January 2021: I was the proud new owner of an RTX 3090 "Founder's Edition" after playing the stock-checking "game" for close to four months. Installation went well, and I was impressed with the performance uplift. However, I was less than impressed by some of the thermals and noise I was seeing and hearing. Memory junction temperatures were north of 100 degrees celsius, and the fan was spinning up *very* hard to compensate. Not exactly the premium $1499 experience I expected.

In their lust for performance, Nvidia's engineers elected to use "GDDR6X"[^1] on RTX 30 series flagship cards, and a *lot* of it. 24 whole gigabytes, in fact, on my fully-populated RTX 3090. Here's where the problems begin. G6X modules only come in 1GB (8Gb) increments, which means Nvidia had to squeeze 24 of these things onto the 3090's PCB. Limitations on board space suck, but the 3090 is a chunky boi. They had to put some modules on the reverse side of the PCB, but that isn't unheard of.

Normally this would be an interesting footnote in a PCB analysis, but the fact that the 24 memory chips are pulling somewhere in the neighborhood of 60-70 watts of power in total turns this from a curiosity into a Big Fucking Problemâ„¢. See, the 3090 "Founder's Edition" has a fatal design flaw - it's too powerful! No, seriously. The 3090 FE has a serious (albeit solvable) thermal management problem when it comes to memory, which Nvidia not only refuses to acknowledge, but actively gaslights their consumers about.

## Analysis

Let's take a look at how the card behaves in an "average" gaming build, running complete default setting with regards to clocks, power targets, and fan curves. I used 3DMark Port Royal's stress test mode to grab these results, and each run was after a cold boot where the computer had been completely unpowered for ~30 minutes beforehand. I ran the card through 5 loops of the stress test mode, and then took a screenshot of HWinfo64 to grab the highest recorded temperatures. [^2]

<img
  src="/images/3090-hwinfo-stock.png"
  alt="HWinfo64 data for a stock 3090 showing poor memory temperatures">
  
> Notice the third column, fourth row: 102 degrees Celsius. This is the T<sub>j</sub> of the hottest memory chip on the board. 

This is where the aforementioned gaslighting starts: the only spec that Micron lists on their [data sheet](https://media-www.micron.com/-/media/client/global/documents/products/data-sheet/dram/gddr/gddr6/gddr6x_sgram_8gb_brief.pdf?rev=161547726f0b45239d3da37ef29b09bf) is T<sub>case</sub>. They state that this should not go above 95 degrees. They **don't** state how to measure T<sub>c</sub>, but maybe there's some standard everyone uses which I'm not privy to as a lowly software developer. I would hazard a guess that it involves thermal paste, kapton tape, and a k-type thermocouple, the latter two I don't posses and have no intention of buying at the moment.

Regardless, Nvidia[^3] naturally took this distinction and [ran with it](https://forums.developer.nvidia.com/t/request-gpu-memory-junction-temperature-via-nvidia-smi-or-nvml-api/168346/160). They claim that the memory junction temperatures visible in Windows are exposed via an "undocumented" API, (presumably) might be reporting incorrect values, and that since T<sub>j</sub> != T<sub>case</sub>, there is no issue with the RTX 3090.

Full disclosure - I don't have any formal education in electronics. I'm not an EE, I don't even have a college degree. Even with that lack of knowledge, I do know one thing about electronics: they aren't magic. They're manufactured components just like anything else, and the same principles apply. You can't run something close to redline for most of its life, and expect it to last. Thermal stress, under/over-voltage, etc.. every operating parameter of a component affects its expected lifespan.

Ultimately what it comes down to is this: 

#### I might be "buy a $1500 GPU" privileged, but I'm not "throw away a $1500 GPU" privileged.

I have to imagine most people using flagship GPUs fit into this category. Perhaps I'm projecting, and people like me aren't as big of a market as I think. Maybe it has no actual effect on Nvidia's bottom line. However, I do know that this GPU demand can't last forever. I have to wonder who Nvidia is planning on selling their consumer cards to once they finally succeed at annoying every nerd on the planet, and the crypto folks have moved onto PoS or ASICs.

*sigh.*

I had originally planned to make this one big post, but this is already getting pretty long and right here feels like a good stopping point. In the next post I'll walk through solving the problem, and some interesting observations I made along the way. It should be significantly less text-heavy and less rant-y when compared to this post. Though it won't be light on Nvidia-dunking, so maybe scratch that last part.

\- snepi

[^1]: It's not JEDEC, so I'm quoting it. @ me about it.
[^2]: I suppose I could have run it through more loops, but after about 5 runs (10 minutes) the card appeared to be pretty close to steady state thermals. Plus I was anxious to get my results.
[^3]: By all indications this response is written by a driver developer at Nvidia. IMO it's completely fair to say "Nvidia" is saying this, it's not some customer service rep who is in over their head.

